\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\begin{document}

\title{Hierarchical Binomials Using an Approximate Infinite Mixture of Betas}
\author{Jacob Carey}

\maketitle

\section{Introduction}

When we have binomial data from different (but related) sources, we often want to share information between them. However, differences between the sources causes a false reduction in variance to pool the data together, and modeling them separately loses information. In this instance, I was interested in the variance between the sources. A typical model choice is a \textit{Beta-Binomial model} written

\begin{equation}\label{betabinom}
\theta_i \sim \text{Beta}(\alpha, \beta)
\end{equation}

where the variance between the souces follows the definition of a Beta distribution with the approximated $\alpha, \beta$ parameters. However, the problem with \eqref{betabinom} is that we force the typically uni-modal assumption on the underylying distribution of the $\theta$'s. A natural extension is to use a \textit{mixture of Beta} distributions for the hierarchical distribution.

\begin{equation}\label{finitemix-betabinom}
\theta_i \sim \sum_i^K pi_i \times \text{Beta}(\alpha_k, \beta_k)
\end{equation}

\eqref{finitemix-betabinom} allows for a more flexible hierarchical distribution. However, we have now introduced a different problem - how do you decide on $K$?

\section{Methods}

\section{Discussion}

\end{document}
